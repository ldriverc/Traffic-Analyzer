{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ldriverc/colab/blob/main/trafficanalyzer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5xt-Kv0pgwT"
      },
      "outputs": [],
      "source": [
        "# 1. INSTALACIÃ“N DE DEPENDENCIAS\n",
        "!pip install pandas numpy matplotlib seaborn plotly scikit-learn\n",
        "!pip install openpyxl xlrd\n",
        "from google.colab import drive, files\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from sklearn.ensemble import RandomForestRegressor, IsolationForest\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "import glob\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AquÃ­ viene el cÃ³digo principal"
      ],
      "metadata": {
        "id": "gjolQV7tqJt-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. CONFIGURACIÃ“N Y CONEXIÃ“N A DRIVE\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "class TrafficAnalyzer:\n",
        "    def __init__(self, base_path='/content/drive/MyDrive/Mediciones'):\n",
        "        self.base_path = base_path\n",
        "        self.data = None\n",
        "        self.peak_hours_morning = (7, 9)\n",
        "        self.peak_hours_evening = (17, 20)\n",
        "        self.scaler = StandardScaler()\n",
        "\n",
        "    def load_all_excel_files(self):\n",
        "        \"\"\"Carga todos los archivos Excel de las carpetas especificadas\"\"\"\n",
        "        print(\"ðŸ”„ Cargando archivos Excel...\")\n",
        "\n",
        "        all_data = []\n",
        "        folders_processed = 0\n",
        "\n",
        "        # Buscar recursivamente todos los archivos Excel\n",
        "        for root, dirs, files in os.walk(self.base_path):\n",
        "            excel_files = [f for f in files if f.endswith(('.xlsx', '.xls'))]\n",
        "\n",
        "            if excel_files:\n",
        "                print(f\"ðŸ“ Procesando carpeta: {root}\")\n",
        "                folders_processed += 1\n",
        "\n",
        "                for file in excel_files:\n",
        "                    file_path = os.path.join(root, file)\n",
        "                    try:\n",
        "                        # Leer el archivo Excel\n",
        "                        df = pd.read_excel(file_path)\n",
        "\n",
        "                        # Agregar metadatos del archivo\n",
        "                        df['archivo'] = file\n",
        "                        df['carpeta'] = os.path.basename(root)\n",
        "                        df['ruta_completa'] = file_path\n",
        "\n",
        "                        all_data.append(df)\n",
        "                        print(f\"  âœ… {file}: {len(df)} registros\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"  âŒ Error al leer {file}: {str(e)}\")\n",
        "\n",
        "        if all_data:\n",
        "            self.data = pd.concat(all_data, ignore_index=True)\n",
        "            print(f\"\\nðŸŽ‰ Datos cargados exitosamente!\")\n",
        "            print(f\"ðŸ“Š Total de registros: {len(self.data)}\")\n",
        "            print(f\"ðŸ“ Carpetas procesadas: {folders_processed}\")\n",
        "\n",
        "            # Preprocessing bÃ¡sico\n",
        "            self._preprocess_data()\n",
        "        else:\n",
        "            print(\"âŒ No se encontraron archivos Excel en la ruta especificada\")\n",
        "\n",
        "    def _preprocess_data(self):\n",
        "        \"\"\"Preprocesamiento de datos\"\"\"\n",
        "        print(\"\\nðŸ”§ Preprocesando datos...\")\n",
        "\n",
        "        # ESTO ES MUY IMPORTANTE: Convertir columnas a tipos apropiados, sino no se leerÃ¡ correctamente\n",
        "        self.data['Fecha'] = pd.to_datetime(self.data['Fecha'], errors='coerce')\n",
        "\n",
        "        # Manejar diferentes formatos de hora\n",
        "        if self.data['Hora'].dtype == 'object':\n",
        "            self.data['Hora'] = pd.to_datetime(self.data['Hora'], format='%H:%M:%S', errors='coerce').dt.time\n",
        "\n",
        "        # Eliminar filas con valores NaT en 'Fecha' antes de combinar\n",
        "        initial_rows = len(self.data)\n",
        "        self.data.dropna(subset=['Fecha'], inplace=True)\n",
        "        rows_dropped = initial_rows - len(self.data)\n",
        "        if rows_dropped > 0:\n",
        "            print(f\"âš ï¸ Se eliminaron {rows_dropped} filas con valores NaT en 'Fecha'.\")\n",
        "\n",
        "        # Crear columna datetime combinada\n",
        "        self.data['datetime'] = pd.to_datetime(\n",
        "            self.data['Fecha'].astype(str) + ' ' + self.data['Hora'].astype(str)\n",
        "        )\n",
        "\n",
        "        # Extraer caracterÃ­sticas temporales\n",
        "        self.data['aÃ±o'] = self.data['Fecha'].dt.year\n",
        "        self.data['mes'] = self.data['Fecha'].dt.month\n",
        "        self.data['dia'] = self.data['Fecha'].dt.day\n",
        "        self.data['hora_num'] = self.data['datetime'].dt.hour\n",
        "        self.data['minuto'] = self.data['datetime'].dt.minute\n",
        "        self.data['dia_semana'] = self.data['Fecha'].dt.dayofweek\n",
        "        self.data['es_fin_semana'] = self.data['dia_semana'].isin([5, 6])\n",
        "\n",
        "        # Identificar horas punta\n",
        "        self.data['es_hora_punta'] = (\n",
        "            ((self.data['hora_num'] >= self.peak_hours_morning[0]) &\n",
        "             (self.data['hora_num'] < self.peak_hours_morning[1])) |\n",
        "            ((self.data['hora_num'] >= self.peak_hours_evening[0]) &\n",
        "             (self.data['hora_num'] < self.peak_hours_evening[1]))\n",
        "        )\n",
        "\n",
        "        # Limpiar datos atÃ­picos con Isolation Forest\n",
        "        self._detect_outliers()\n",
        "\n",
        "        print(f\"âœ… Preprocesamiento completado\")\n",
        "        print(f\"ðŸ“… Rango de fechas: {self.data['Fecha'].min()} - {self.data['Fecha'].max()}\")\n",
        "        print(f\"ðŸš— Pistas Ãºnicas: {self.data['Pista'].nunique()}\")\n",
        "\n",
        "    def _detect_outliers(self):\n",
        "        \"\"\"Detecta y marca outliers usando Isolation Forest\"\"\"\n",
        "        print(\"ðŸ” Detectando valores atÃ­picos...\")\n",
        "\n",
        "        # Aplicar Isolation Forest\n",
        "        iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
        "        features = ['Flujo', 'OcupaciÃ³n', 'hora_num', 'Pista']\n",
        "\n",
        "        # Handle potential non-numeric 'Pista' values or missing values\n",
        "        temp_data = self.data.copy()\n",
        "        temp_data['Pista'] = pd.to_numeric(temp_data['Pista'], errors='coerce')\n",
        "        temp_data.dropna(subset=features, inplace=True)\n",
        "\n",
        "\n",
        "        if not temp_data.empty:\n",
        "            outliers = iso_forest.fit_predict(temp_data[features])\n",
        "            # Align outliers back to the original dataframe index\n",
        "            self.data['es_outlier'] = False # Initialize all as not outlier\n",
        "            self.data.loc[temp_data.index, 'es_outlier'] = outliers == -1\n",
        "        else:\n",
        "            print(\"âš ï¸ No hay datos vÃ¡lidos para la detecciÃ³n de outliers.\")\n",
        "            self.data['es_outlier'] = False\n",
        "\n",
        "\n",
        "        outlier_count = self.data['es_outlier'].sum()\n",
        "        print(f\"ðŸš¨ Outliers detectados: {outlier_count} ({outlier_count/len(self.data)*100:.2f}%)\")\n",
        "\n",
        "\n",
        "    def analyze_peak_hours_comparison(self):\n",
        "        \"\"\"AnÃ¡lisis comparativo de horas punta entre 2024 y 2025\"\"\"\n",
        "        print(\"\\nðŸ“ˆ ANÃLISIS COMPARATIVO HORAS PUNTA 2024 vs 2025\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Filtrar datos de horas punta sin outliers\n",
        "        peak_data = self.data[\n",
        "            (self.data['es_hora_punta'] == True) &\n",
        "            (self.data['es_outlier'] == False) &\n",
        "            (self.data['aÃ±o'].isin([2024, 2025]))\n",
        "        ]\n",
        "\n",
        "        if len(peak_data) == 0:\n",
        "            print(\"âŒ No hay datos de horas punta para 2024 y 2025\")\n",
        "            return\n",
        "\n",
        "        # AnÃ¡lisis por aÃ±o\n",
        "        results = {}\n",
        "        for year in [2024, 2025]:\n",
        "            year_data = peak_data[peak_data['aÃ±o'] == year]\n",
        "\n",
        "            if len(year_data) > 0:\n",
        "                results[year] = {\n",
        "                    'flujo_promedio': year_data['Flujo'].mean(),\n",
        "                    'ocupacion_promedio': year_data['OcupaciÃ³n'].mean(),\n",
        "                    'flujo_std': year_data['Flujo'].std(),\n",
        "                    'ocupacion_std': year_data['OcupaciÃ³n'].std(),\n",
        "                    'registros': len(year_data),\n",
        "                    'pistas_unicas': year_data['Pista'].nunique()\n",
        "                }\n",
        "\n",
        "        # Mostrar resultados\n",
        "        for year, stats in results.items():\n",
        "            print(f\"\\nðŸ“Š AÃ‘O {year}:\")\n",
        "            print(f\"   ðŸš— Flujo promedio: {stats['flujo_promedio']:.2f} Â± {stats['flujo_std']:.2f}\")\n",
        "            print(f\"   â±ï¸  OcupaciÃ³n promedio: {stats['ocupacion_promedio']:.2f} Â± {stats['ocupacion_std']:.2f} ms\")\n",
        "            print(f\"   ðŸ“‹ Registros: {stats['registros']}\")\n",
        "            print(f\"   ðŸ›£ï¸  Pistas Ãºnicas: {stats['pistas_unicas']}\")\n",
        "\n",
        "        # AnÃ¡lisis de cambios\n",
        "        if 2024 in results and 2025 in results:\n",
        "            print(f\"\\nðŸ”„ CAMBIOS 2024 â†’ 2025:\")\n",
        "\n",
        "            flujo_change = ((results[2025]['flujo_promedio'] - results[2024]['flujo_promedio']) /\n",
        "                           results[2024]['flujo_promedio'] * 100)\n",
        "            ocupacion_change = ((results[2025]['ocupacion_promedio'] - results[2024]['ocupacion_promedio']) /\n",
        "                               results[2024]['ocupacion_promedio'] * 100)\n",
        "\n",
        "            print(f\"   ðŸš— Cambio en flujo: {flujo_change:+.2f}%\")\n",
        "            print(f\"   â±ï¸  Cambio en ocupaciÃ³n: {ocupacion_change:+.2f}%\")\n",
        "\n",
        "            # InterpretaciÃ³n\n",
        "            if flujo_change > 5:\n",
        "                print(\"   ðŸ“ˆ AUMENTO SIGNIFICATIVO en el flujo vehicular\")\n",
        "            elif flujo_change < -5:\n",
        "                print(\"   ðŸ“‰ DISMINUCIÃ“N SIGNIFICATIVA en el flujo vehicular\")\n",
        "            else:\n",
        "                print(\"   âž¡ï¸  Flujo vehicular ESTABLE\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def ml_traffic_prediction(self):\n",
        "        \"\"\"Modelo de ML para predicciÃ³n de trÃ¡fico\"\"\"\n",
        "        print(\"\\nðŸ¤– MODELO DE MACHINE LEARNING PARA PREDICCIÃ“N\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Preparar datos para ML\n",
        "        ml_data = self.data[self.data['es_outlier'] == False].copy()\n",
        "\n",
        "        # Features engineering\n",
        "        features = [\n",
        "            'hora_num', 'minuto', 'dia_semana', 'mes', 'dia',\n",
        "            'Pista', 'es_hora_punta', 'es_fin_semana'\n",
        "        ]\n",
        "\n",
        "        # Ensure 'Pista' is numeric before one-hot encoding\n",
        "        ml_data['Pista'] = pd.to_numeric(ml_data['Pista'], errors='coerce')\n",
        "        ml_data.dropna(subset=['Pista'], inplace=True)\n",
        "\n",
        "\n",
        "        # Crear variables dummy para categÃ³ricas\n",
        "        ml_data_encoded = pd.get_dummies(ml_data, columns=['Pista'], prefix='pista')\n",
        "\n",
        "        # Actualizar features con las nuevas columnas\n",
        "        pista_cols = [col for col in ml_data_encoded.columns if col.startswith('pista_')]\n",
        "        features_final = ['hora_num', 'minuto', 'dia_semana', 'mes', 'dia',\n",
        "                         'es_hora_punta', 'es_fin_semana'] + pista_cols\n",
        "\n",
        "        # Ensure all features exist and are numeric\n",
        "        for col in features_final:\n",
        "            if col not in ml_data_encoded.columns:\n",
        "                ml_data_encoded[col] = 0\n",
        "            else:\n",
        "                ml_data_encoded[col] = pd.to_numeric(ml_data_encoded[col], errors='coerce')\n",
        "\n",
        "        ml_data_encoded.dropna(subset=features_final, inplace=True)\n",
        "\n",
        "\n",
        "        X = ml_data_encoded[features_final]\n",
        "        y_flujo = ml_data_encoded['Flujo']\n",
        "        y_ocupacion = ml_data_encoded['OcupaciÃ³n']\n",
        "\n",
        "        # Check if there's enough data after cleaning\n",
        "        if len(X) == 0:\n",
        "            print(\"âŒ No hay suficientes datos vÃ¡lidos para entrenar el modelo de ML.\")\n",
        "            return None, None, None\n",
        "\n",
        "\n",
        "        # Dividir datos\n",
        "        X_train, X_test, y_flujo_train, y_flujo_test, y_ocupacion_train, y_ocupacion_test = train_test_split(\n",
        "            X, y_flujo, y_ocupacion, test_size=0.2, random_state=42\n",
        "        )\n",
        "\n",
        "        # Entrenar modelos\n",
        "        print(\"ðŸŽ¯ Entrenando modelos Random Forest...\")\n",
        "\n",
        "        # Modelo para flujo\n",
        "        rf_flujo = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "        rf_flujo.fit(X_train, y_flujo_train)\n",
        "\n",
        "        # Modelo para ocupaciÃ³n\n",
        "        rf_ocupacion = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "        rf_ocupacion.fit(X_train, y_ocupacion_train)\n",
        "\n",
        "        # Predicciones\n",
        "        y_flujo_pred = rf_flujo.predict(X_test)\n",
        "        y_ocupacion_pred = rf_ocupacion.predict(X_test)\n",
        "\n",
        "        # MÃ©tricas\n",
        "        print(f\"\\nðŸ“Š MÃ‰TRICAS DEL MODELO:\")\n",
        "        print(f\"   ðŸš— FLUJO:\")\n",
        "        print(f\"      RÂ²: {r2_score(y_flujo_test, y_flujo_pred):.4f}\")\n",
        "        print(f\"      MAE: {mean_absolute_error(y_flujo_test, y_flujo_pred):.4f}\")\n",
        "        print(f\"      RMSE: {np.sqrt(mean_squared_error(y_flujo_test, y_flujo_pred)):.4f}\")\n",
        "\n",
        "        print(f\"   â±ï¸  OCUPACIÃ“N:\")\n",
        "        print(f\"      RÂ²: {r2_score(y_ocupacion_test, y_ocupacion_pred):.4f}\")\n",
        "        print(f\"      MAE: {mean_absolute_error(y_ocupacion_test, y_ocupacion_pred):.4f}\")\n",
        "        print(f\"      RMSE: {np.sqrt(mean_squared_error(y_ocupacion_test, y_ocupacion_pred)):.4f}\")\n",
        "\n",
        "        # Importancia de features\n",
        "        self._plot_feature_importance(rf_flujo, features_final, \"Flujo\")\n",
        "        self._plot_feature_importance(rf_ocupacion, features_final, \"OcupaciÃ³n\")\n",
        "\n",
        "        return rf_flujo, rf_ocupacion, features_final\n",
        "\n",
        "    def _plot_feature_importance(self, model, features, target_name):\n",
        "        \"\"\"Visualiza la importancia de las caracterÃ­sticas\"\"\"\n",
        "        importances = model.feature_importances_\n",
        "        indices = np.argsort(importances)[::-1]\n",
        "\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.title(f'Importancia de CaracterÃ­sticas - {target_name}')\n",
        "        plt.bar(range(len(importances)), importances[indices])\n",
        "        plt.xticks(range(len(importances)), [features[i] for i in indices], rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def clustering_analysis(self):\n",
        "        \"\"\"AnÃ¡lisis de clustering para identificar patrones\"\"\"\n",
        "        print(\"\\nðŸ” ANÃLISIS DE CLUSTERING - PATRONES DE TRÃFICO\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Preparar datos para clustering\n",
        "        cluster_data = self.data[self.data['es_outlier'] == False].copy()\n",
        "\n",
        "        # Ensure 'Pista' is numeric before grouping\n",
        "        cluster_data['Pista'] = pd.to_numeric(cluster_data['Pista'], errors='coerce')\n",
        "        cluster_data.dropna(subset=['Pista'], inplace=True)\n",
        "\n",
        "\n",
        "        # Agrupar por hora y calcular promedios\n",
        "        hourly_patterns = cluster_data.groupby(['hora_num', 'Pista']).agg({\n",
        "            'Flujo': 'mean',\n",
        "            'OcupaciÃ³n': 'mean'\n",
        "        }).reset_index()\n",
        "\n",
        "        if hourly_patterns.empty:\n",
        "             print(\"âŒ No hay suficientes datos vÃ¡lidos para el anÃ¡lisis de clustering.\")\n",
        "             return None, None\n",
        "\n",
        "\n",
        "        # Aplicar K-means\n",
        "        features_cluster = ['hora_num', 'Flujo', 'OcupaciÃ³n']\n",
        "        X_cluster = hourly_patterns[features_cluster]\n",
        "\n",
        "        # Normalizar datos\n",
        "        X_cluster_scaled = self.scaler.fit_transform(X_cluster)\n",
        "\n",
        "        # Encontrar nÃºmero Ã³ptimo de clusters (mÃ©todo del codo)\n",
        "        inertias = []\n",
        "        K_range = range(2, min(10, len(X_cluster_scaled) + 1)) # Adjust K_range based on data size\n",
        "\n",
        "\n",
        "        for k in K_range:\n",
        "            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10) # Add n_init\n",
        "            kmeans.fit(X_cluster_scaled)\n",
        "            inertias.append(kmeans.inertia_)\n",
        "\n",
        "        # Aplicar clustering con k Ã³ptimo (asumimos k=4 para este ejemplo)\n",
        "        optimal_k = 4\n",
        "        if len(X_cluster_scaled) < optimal_k:\n",
        "             print(f\"âš ï¸ No hay suficientes puntos de datos para {optimal_k} clusters. Usando {len(X_cluster_scaled)} clusters.\")\n",
        "             optimal_k = len(X_cluster_scaled)\n",
        "             if optimal_k < 2:\n",
        "                 print(\"âŒ No hay suficientes datos para realizar clustering.\")\n",
        "                 return None, None\n",
        "\n",
        "        kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10) # Add n_init\n",
        "        hourly_patterns['cluster'] = kmeans.fit_predict(X_cluster_scaled)\n",
        "\n",
        "        # Visualizar clusters\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "        fig.suptitle('AnÃ¡lisis de Clustering - Patrones de TrÃ¡fico', fontsize=16)\n",
        "\n",
        "        # MÃ©todo del codo\n",
        "        axes[0, 0].plot(K_range, inertias, 'bo-')\n",
        "        axes[0, 0].set_xlabel('NÃºmero de Clusters')\n",
        "        axes[0, 0].set_ylabel('Inercia')\n",
        "        axes[0, 0].set_title('MÃ©todo del Codo')\n",
        "        axes[0, 0].grid(True)\n",
        "\n",
        "        # Scatter plot de clusters\n",
        "        scatter = axes[0, 1].scatter(hourly_patterns['Flujo'], hourly_patterns['OcupaciÃ³n'],\n",
        "                                    c=hourly_patterns['cluster'], cmap='viridis')\n",
        "        axes[0, 1].set_xlabel('Flujo Promedio')\n",
        "        axes[0, 1].set_ylabel('OcupaciÃ³n Promedio')\n",
        "        axes[0, 1].set_title('Clusters de TrÃ¡fico')\n",
        "        plt.colorbar(scatter, ax=axes[0, 1])\n",
        "\n",
        "        # Patrones por hora\n",
        "        for cluster in range(optimal_k):\n",
        "            cluster_data_plot = hourly_patterns[hourly_patterns['cluster'] == cluster]\n",
        "            axes[1, 0].plot(cluster_data_plot['hora_num'], cluster_data_plot['Flujo'],\n",
        "                           label=f'Cluster {cluster}', marker='o')\n",
        "\n",
        "        axes[1, 0].set_xlabel('Hora del dÃ­a')\n",
        "        axes[1, 0].set_ylabel('Flujo Promedio')\n",
        "        axes[1, 0].set_title('Patrones de Flujo por Cluster')\n",
        "        axes[1, 0].legend()\n",
        "        axes[1, 0].grid(True)\n",
        "\n",
        "        # DistribuciÃ³n de clusters\n",
        "        cluster_counts = hourly_patterns['cluster'].value_counts().sort_index()\n",
        "        axes[1, 1].bar(cluster_counts.index, cluster_counts.values)\n",
        "        axes[1, 1].set_xlabel('Cluster')\n",
        "        axes[1, 1].set_ylabel('Cantidad de Puntos')\n",
        "        axes[1, 1].set_title('DistribuciÃ³n de Clusters')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # AnÃ¡lisis de caracterÃ­sticas de cada cluster\n",
        "        print(\"\\nðŸ“‹ CARACTERÃSTICAS DE CADA CLUSTER:\")\n",
        "        for cluster in range(optimal_k):\n",
        "            cluster_data = hourly_patterns[hourly_patterns['cluster'] == cluster]\n",
        "            print(f\"\\nðŸ·ï¸  CLUSTER {cluster}:\")\n",
        "            print(f\"   ðŸ“Š Puntos: {len(cluster_data)}\")\n",
        "            print(f\"   ðŸš— Flujo promedio: {cluster_data['Flujo'].mean():.2f}\")\n",
        "            print(f\"   â±ï¸  OcupaciÃ³n promedio: {cluster_data['OcupaciÃ³n'].mean():.2f}\")\n",
        "            print(f\"   ðŸ• Horas tÃ­picas: {cluster_data['hora_num'].min()}-{cluster_data['hora_num'].max()}\")\n",
        "\n",
        "        return hourly_patterns, kmeans\n",
        "\n",
        "    def generate_comprehensive_report(self):\n",
        "        \"\"\"Genera un reporte completo con visualizaciones\"\"\"\n",
        "        print(\"\\nðŸ“Š GENERANDO REPORTE COMPLETO\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Resumen general\n",
        "        print(f\"ðŸ“‹ RESUMEN GENERAL:\")\n",
        "        print(f\"   ðŸ“… PerÃ­odo: {self.data['Fecha'].min()} - {self.data['Fecha'].max()}\")\n",
        "        print(f\"   ðŸ“Š Total registros: {len(self.data)}\")\n",
        "        print(f\"   ðŸ›£ï¸  Pistas analizadas: {self.data['Pista'].nunique()}\")\n",
        "        print(f\"   ðŸš¨ Outliers detectados: {self.data['es_outlier'].sum()}\")\n",
        "\n",
        "        # EstadÃ­sticas por aÃ±o\n",
        "        yearly_stats = self.data.groupby('aÃ±o').agg({\n",
        "            'Flujo': ['mean', 'std', 'min', 'max'],\n",
        "            'OcupaciÃ³n': ['mean', 'std', 'min', 'max']\n",
        "        }).round(2)\n",
        "\n",
        "        print(f\"\\nðŸ“ˆ ESTADÃSTICAS POR AÃ‘O:\")\n",
        "        print(yearly_stats)\n",
        "\n",
        "        # Crear visualizaciones\n",
        "        self._create_visualizations()\n",
        "\n",
        "    def _create_visualizations(self):\n",
        "        \"\"\"Crea visualizaciones comprehensivas\"\"\"\n",
        "\n",
        "        # Configurar el estilo\n",
        "        plt.style.use('seaborn-v0_8')\n",
        "\n",
        "        # 1. DistribuciÃ³n temporal del trÃ¡fico\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(20, 15))\n",
        "        fig.suptitle('AnÃ¡lisis Temporal del TrÃ¡fico Vehicular', fontsize=16)\n",
        "\n",
        "        # Flujo por hora del dÃ­a\n",
        "        hourly_flow = self.data.groupby('hora_num')['Flujo'].mean()\n",
        "        axes[0, 0].plot(hourly_flow.index, hourly_flow.values, marker='o', linewidth=2)\n",
        "        axes[0, 0].axvspan(7, 9, alpha=0.3, color='red', label='Hora punta maÃ±ana')\n",
        "        axes[0, 0].axvspan(17, 20, alpha=0.3, color='red', label='Hora punta tarde')\n",
        "        axes[0, 0].set_xlabel('Hora del dÃ­a')\n",
        "        axes[0, 0].set_ylabel('Flujo promedio')\n",
        "        axes[0, 0].set_title('Flujo vehicular por hora del dÃ­a')\n",
        "        axes[0, 0].legend()\n",
        "        axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        # OcupaciÃ³n por hora del dÃ­a\n",
        "        hourly_occ = self.data.groupby('hora_num')['OcupaciÃ³n'].mean()\n",
        "        axes[0, 1].plot(hourly_occ.index, hourly_occ.values, marker='s', linewidth=2, color='orange')\n",
        "        axes[0, 1].axvspan(7, 9, alpha=0.3, color='red')\n",
        "        axes[0, 1].axvspan(17, 20, alpha=0.3, color='red')\n",
        "        axes[0, 1].set_xlabel('Hora del dÃ­a')\n",
        "        axes[0, 1].set_ylabel('OcupaciÃ³n promedio (ms)')\n",
        "        axes[0, 1].set_title('OcupaciÃ³n por hora del dÃ­a')\n",
        "        axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "        # Flujo por dÃ­a de la semana\n",
        "        daily_flow = self.data.groupby('dia_semana')['Flujo'].mean()\n",
        "        days = ['Lun', 'Mar', 'MiÃ©', 'Jue', 'Vie', 'SÃ¡b', 'Dom']\n",
        "        axes[1, 0].bar(daily_flow.index, daily_flow.values, color='skyblue') # Use index for positions\n",
        "        axes[1, 0].set_xticks(daily_flow.index) # Set tick positions\n",
        "        axes[1, 0].set_xticklabels(days) # Set tick labels\n",
        "        axes[1, 0].set_xlabel('DÃ­a de la semana')\n",
        "        axes[1, 0].set_ylabel('Flujo promedio')\n",
        "        axes[1, 0].set_title('Flujo vehicular por dÃ­a de la semana')\n",
        "        axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        # ComparaciÃ³n por pista\n",
        "        pista_stats = self.data.groupby('Pista')['Flujo'].mean().sort_values(ascending=False)\n",
        "        axes[1, 1].bar(pista_stats.index.astype(str), pista_stats.values, color='lightgreen')\n",
        "        axes[1, 1].set_xlabel('Pista')\n",
        "        axes[1, 1].set_ylabel('Flujo promedio')\n",
        "        axes[1, 1].set_title('Flujo promedio por pista')\n",
        "        axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # 2. Heatmap de trÃ¡fico por hora y dÃ­a\n",
        "        pivot_data = self.data.pivot_table(\n",
        "            values='Flujo',\n",
        "            index='hora_num',\n",
        "            columns='dia_semana',\n",
        "            aggfunc='mean'\n",
        "        )\n",
        "\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        sns.heatmap(pivot_data, annot=True, fmt='.1f', cmap='YlOrRd',\n",
        "                   xticklabels=days, yticklabels=range(24))\n",
        "        plt.title('Heatmap: Flujo vehicular por hora y dÃ­a de la semana')\n",
        "        plt.xlabel('DÃ­a de la semana')\n",
        "        plt.ylabel('Hora del dÃ­a')\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNFxXPORp5b2",
        "outputId": "bde0a65d-e2f7-41a7-b49f-427771163381"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. FUNCIÃ“N PRINCIPAL DE EJECUCIÃ“N\n",
        "def main():\n",
        "    \"\"\"FunciÃ³n principal que ejecuta todo el anÃ¡lisis\"\"\"\n",
        "    print(\"ðŸš€ INICIANDO ANÃLISIS DE TRÃFICO VEHICULAR CON ML\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Inicializar analizador\n",
        "    analyzer = TrafficAnalyzer()\n",
        "\n",
        "    # Cargar datos\n",
        "    analyzer.load_all_excel_files()\n",
        "\n",
        "    if analyzer.data is None:\n",
        "        print(\"âŒ No se pudieron cargar los datos. Verifica la ruta.\")\n",
        "        return\n",
        "\n",
        "    # Ejecutar anÃ¡lisis\n",
        "    print(\"\\nðŸ” EJECUTANDO ANÃLISIS...\")\n",
        "\n",
        "    # 1. AnÃ¡lisis comparativo bÃ¡sico\n",
        "    comparison_results = analyzer.analyze_peak_hours_comparison()\n",
        "\n",
        "    # 2. Modelo de ML\n",
        "    rf_flujo, rf_ocupacion, features = analyzer.ml_traffic_prediction()\n",
        "\n",
        "    # 3. AnÃ¡lisis de clustering\n",
        "    patterns, kmeans_model = analyzer.clustering_analysis()\n",
        "\n",
        "    # 4. Reporte completo\n",
        "    analyzer.generate_comprehensive_report()\n",
        "\n",
        "    print(\"\\nâœ… ANÃLISIS COMPLETADO\")\n",
        "    print(\"ðŸ“Š Todos los resultados han sido generados exitosamente\")\n",
        "\n",
        "    return analyzer, rf_flujo, rf_ocupacion, patterns, kmeans_model"
      ],
      "metadata": {
        "id": "VQ9uzhUjpzYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. FUNCIÃ“N DE PREDICCIÃ“N PARA NUEVOS DATOS\n",
        "def predict_traffic(analyzer, rf_flujo, rf_ocupacion, features, hora, pista, dia_semana, mes=1):\n",
        "    \"\"\"FunciÃ³n para predecir trÃ¡fico en condiciones especÃ­ficas\"\"\"\n",
        "    # Crear DataFrame con las caracterÃ­sticas\n",
        "    new_data = pd.DataFrame({\n",
        "        'hora_num': [hora],\n",
        "        'minuto': [0],\n",
        "        'dia_semana': [dia_semana],\n",
        "        'mes': [mes],\n",
        "        'dia': [1],\n",
        "        'es_hora_punta': [hora in range(7, 9) or hora in range(17, 20)],\n",
        "        'es_fin_semana': [dia_semana in [5, 6]]\n",
        "    })\n",
        "\n",
        "    # Agregar columnas de pista (one-hot encoding)\n",
        "    for feature in features:\n",
        "        if feature.startswith('pista_'):\n",
        "            pista_num = feature.split('_')[1]\n",
        "            new_data[feature] = [1 if pista_num == str(pista) else 0]\n",
        "\n",
        "    # Asegurar que tenemos todas las columnas necesarias\n",
        "    for feature in features:\n",
        "        if feature not in new_data.columns:\n",
        "            new_data[feature] = [0]\n",
        "\n",
        "    # Ordenar columnas segÃºn el orden de entrenamiento\n",
        "    new_data = new_data[features]\n",
        "\n",
        "    # Realizar predicciÃ³n\n",
        "    flujo_pred = rf_flujo.predict(new_data)[0]\n",
        "    ocupacion_pred = rf_ocupacion.predict(new_data)[0]\n",
        "\n",
        "    return flujo_pred, ocupacion_pred"
      ],
      "metadata": {
        "id": "Bh8OOK7MpwI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. EJECUTAR ANÃLISIS\n",
        "if __name__ == \"__main__\":\n",
        "    # Ejecutar anÃ¡lisis principal\n",
        "    analyzer, rf_flujo, rf_ocupacion, patterns, kmeans_model = main()\n",
        "\n",
        "    # Ejemplo de predicciÃ³n\n",
        "    print(\"\\nðŸ”® EJEMPLO DE PREDICCIÃ“N:\")\n",
        "    flujo_pred, ocupacion_pred = predict_traffic(\n",
        "        analyzer, rf_flujo, rf_ocupacion,\n",
        "        analyzer.ml_traffic_prediction()[2],  # features\n",
        "        hora=8,  # 8 AM\n",
        "        pista=1,\n",
        "        dia_semana=1,  # Martes\n",
        "        mes=7  # Julio\n",
        "    )\n",
        "\n",
        "    print(f\"ðŸ“Š PredicciÃ³n para Pista 1, Martes 8:00 AM:\")\n",
        "    print(f\"   ðŸš— Flujo estimado: {flujo_pred:.2f} vehÃ­culos\")\n",
        "    print(f\"   â±ï¸  OcupaciÃ³n estimada: {ocupacion_pred:.2f} ms\")"
      ],
      "metadata": {
        "id": "g-0sCDFcptOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5de1e5cc"
      },
      "source": [
        "# 5. EJECUTAR ANÃLISIS\n",
        "if __name__ == \"__main__\":\n",
        "    # Ejecutar anÃ¡lisis principal\n",
        "    analyzer, rf_flujo, rf_ocupacion, patterns, kmeans_model = main()\n",
        "\n",
        "    # Ejemplo de predicciÃ³n\n",
        "    print(\"\\nðŸ”® EJEMPLO DE PREDICCIÃ“N:\")\n",
        "    # Ensure features are passed correctly\n",
        "    flujo_pred, ocupacion_pred = predict_traffic(\n",
        "        analyzer, rf_flujo, rf_ocupacion,\n",
        "        analyzer.ml_traffic_prediction()[2],  # features\n",
        "        hora=8,  # 8 AM\n",
        "        pista=1,\n",
        "        dia_semana=1,  # Martes\n",
        "        mes=7  # Julio\n",
        "    )\n",
        "\n",
        "    print(f\"ðŸ“Š PredicciÃ³n para Pista 1, Martes 8:00 AM:\")\n",
        "    print(f\"   ðŸš— Flujo estimado: {flujo_pred:.2f} vehÃ­culos\")\n",
        "    print(f\"   â±ï¸  OcupaciÃ³n estimada: {ocupacion_pred:.2f} ms\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
