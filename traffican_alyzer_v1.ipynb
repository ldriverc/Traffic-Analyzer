{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ldriverc/colab/blob/main/trafficanalyzer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5xt-Kv0pgwT"
      },
      "outputs": [],
      "source": [
        "# 1. INSTALACIÓN DE DEPENDENCIAS\n",
        "!pip install pandas numpy matplotlib seaborn plotly scikit-learn\n",
        "!pip install openpyxl xlrd\n",
        "from google.colab import drive, files\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from sklearn.ensemble import RandomForestRegressor, IsolationForest\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "import glob\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aquí viene el código principal"
      ],
      "metadata": {
        "id": "gjolQV7tqJt-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. CONFIGURACIÓN Y CONEXIÓN A DRIVE\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "class TrafficAnalyzer:\n",
        "    def __init__(self, base_path='/content/drive/MyDrive/Mediciones'):\n",
        "        self.base_path = base_path\n",
        "        self.data = None\n",
        "        self.peak_hours_morning = (7, 9)\n",
        "        self.peak_hours_evening = (17, 20)\n",
        "        self.scaler = StandardScaler()\n",
        "\n",
        "    def load_all_excel_files(self):\n",
        "        \"\"\"Carga todos los archivos Excel de las carpetas especificadas\"\"\"\n",
        "        print(\"🔄 Cargando archivos Excel...\")\n",
        "\n",
        "        all_data = []\n",
        "        folders_processed = 0\n",
        "\n",
        "        # Buscar recursivamente todos los archivos Excel\n",
        "        for root, dirs, files in os.walk(self.base_path):\n",
        "            excel_files = [f for f in files if f.endswith(('.xlsx', '.xls'))]\n",
        "\n",
        "            if excel_files:\n",
        "                print(f\"📁 Procesando carpeta: {root}\")\n",
        "                folders_processed += 1\n",
        "\n",
        "                for file in excel_files:\n",
        "                    file_path = os.path.join(root, file)\n",
        "                    try:\n",
        "                        # Leer el archivo Excel\n",
        "                        df = pd.read_excel(file_path)\n",
        "\n",
        "                        # Agregar metadatos del archivo\n",
        "                        df['archivo'] = file\n",
        "                        df['carpeta'] = os.path.basename(root)\n",
        "                        df['ruta_completa'] = file_path\n",
        "\n",
        "                        all_data.append(df)\n",
        "                        print(f\"  ✅ {file}: {len(df)} registros\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"  ❌ Error al leer {file}: {str(e)}\")\n",
        "\n",
        "        if all_data:\n",
        "            self.data = pd.concat(all_data, ignore_index=True)\n",
        "            print(f\"\\n🎉 Datos cargados exitosamente!\")\n",
        "            print(f\"📊 Total de registros: {len(self.data)}\")\n",
        "            print(f\"📁 Carpetas procesadas: {folders_processed}\")\n",
        "\n",
        "            # Preprocessing básico\n",
        "            self._preprocess_data()\n",
        "        else:\n",
        "            print(\"❌ No se encontraron archivos Excel en la ruta especificada\")\n",
        "\n",
        "    def _preprocess_data(self):\n",
        "        \"\"\"Preprocesamiento de datos\"\"\"\n",
        "        print(\"\\n🔧 Preprocesando datos...\")\n",
        "\n",
        "        # ESTO ES MUY IMPORTANTE: Convertir columnas a tipos apropiados, sino no se leerá correctamente\n",
        "        self.data['Fecha'] = pd.to_datetime(self.data['Fecha'], errors='coerce')\n",
        "\n",
        "        # Manejar diferentes formatos de hora\n",
        "        if self.data['Hora'].dtype == 'object':\n",
        "            self.data['Hora'] = pd.to_datetime(self.data['Hora'], format='%H:%M:%S', errors='coerce').dt.time\n",
        "\n",
        "        # Eliminar filas con valores NaT en 'Fecha' antes de combinar\n",
        "        initial_rows = len(self.data)\n",
        "        self.data.dropna(subset=['Fecha'], inplace=True)\n",
        "        rows_dropped = initial_rows - len(self.data)\n",
        "        if rows_dropped > 0:\n",
        "            print(f\"⚠️ Se eliminaron {rows_dropped} filas con valores NaT en 'Fecha'.\")\n",
        "\n",
        "        # Crear columna datetime combinada\n",
        "        self.data['datetime'] = pd.to_datetime(\n",
        "            self.data['Fecha'].astype(str) + ' ' + self.data['Hora'].astype(str)\n",
        "        )\n",
        "\n",
        "        # Extraer características temporales\n",
        "        self.data['año'] = self.data['Fecha'].dt.year\n",
        "        self.data['mes'] = self.data['Fecha'].dt.month\n",
        "        self.data['dia'] = self.data['Fecha'].dt.day\n",
        "        self.data['hora_num'] = self.data['datetime'].dt.hour\n",
        "        self.data['minuto'] = self.data['datetime'].dt.minute\n",
        "        self.data['dia_semana'] = self.data['Fecha'].dt.dayofweek\n",
        "        self.data['es_fin_semana'] = self.data['dia_semana'].isin([5, 6])\n",
        "\n",
        "        # Identificar horas punta\n",
        "        self.data['es_hora_punta'] = (\n",
        "            ((self.data['hora_num'] >= self.peak_hours_morning[0]) &\n",
        "             (self.data['hora_num'] < self.peak_hours_morning[1])) |\n",
        "            ((self.data['hora_num'] >= self.peak_hours_evening[0]) &\n",
        "             (self.data['hora_num'] < self.peak_hours_evening[1]))\n",
        "        )\n",
        "\n",
        "        # Limpiar datos atípicos con Isolation Forest\n",
        "        self._detect_outliers()\n",
        "\n",
        "        print(f\"✅ Preprocesamiento completado\")\n",
        "        print(f\"📅 Rango de fechas: {self.data['Fecha'].min()} - {self.data['Fecha'].max()}\")\n",
        "        print(f\"🚗 Pistas únicas: {self.data['Pista'].nunique()}\")\n",
        "\n",
        "    def _detect_outliers(self):\n",
        "        \"\"\"Detecta y marca outliers usando Isolation Forest\"\"\"\n",
        "        print(\"🔍 Detectando valores atípicos...\")\n",
        "\n",
        "        # Aplicar Isolation Forest\n",
        "        iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
        "        features = ['Flujo', 'Ocupación', 'hora_num', 'Pista']\n",
        "\n",
        "        # Handle potential non-numeric 'Pista' values or missing values\n",
        "        temp_data = self.data.copy()\n",
        "        temp_data['Pista'] = pd.to_numeric(temp_data['Pista'], errors='coerce')\n",
        "        temp_data.dropna(subset=features, inplace=True)\n",
        "\n",
        "\n",
        "        if not temp_data.empty:\n",
        "            outliers = iso_forest.fit_predict(temp_data[features])\n",
        "            # Align outliers back to the original dataframe index\n",
        "            self.data['es_outlier'] = False # Initialize all as not outlier\n",
        "            self.data.loc[temp_data.index, 'es_outlier'] = outliers == -1\n",
        "        else:\n",
        "            print(\"⚠️ No hay datos válidos para la detección de outliers.\")\n",
        "            self.data['es_outlier'] = False\n",
        "\n",
        "\n",
        "        outlier_count = self.data['es_outlier'].sum()\n",
        "        print(f\"🚨 Outliers detectados: {outlier_count} ({outlier_count/len(self.data)*100:.2f}%)\")\n",
        "\n",
        "\n",
        "    def analyze_peak_hours_comparison(self):\n",
        "        \"\"\"Análisis comparativo de horas punta entre 2024 y 2025\"\"\"\n",
        "        print(\"\\n📈 ANÁLISIS COMPARATIVO HORAS PUNTA 2024 vs 2025\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Filtrar datos de horas punta sin outliers\n",
        "        peak_data = self.data[\n",
        "            (self.data['es_hora_punta'] == True) &\n",
        "            (self.data['es_outlier'] == False) &\n",
        "            (self.data['año'].isin([2024, 2025]))\n",
        "        ]\n",
        "\n",
        "        if len(peak_data) == 0:\n",
        "            print(\"❌ No hay datos de horas punta para 2024 y 2025\")\n",
        "            return\n",
        "\n",
        "        # Análisis por año\n",
        "        results = {}\n",
        "        for year in [2024, 2025]:\n",
        "            year_data = peak_data[peak_data['año'] == year]\n",
        "\n",
        "            if len(year_data) > 0:\n",
        "                results[year] = {\n",
        "                    'flujo_promedio': year_data['Flujo'].mean(),\n",
        "                    'ocupacion_promedio': year_data['Ocupación'].mean(),\n",
        "                    'flujo_std': year_data['Flujo'].std(),\n",
        "                    'ocupacion_std': year_data['Ocupación'].std(),\n",
        "                    'registros': len(year_data),\n",
        "                    'pistas_unicas': year_data['Pista'].nunique()\n",
        "                }\n",
        "\n",
        "        # Mostrar resultados\n",
        "        for year, stats in results.items():\n",
        "            print(f\"\\n📊 AÑO {year}:\")\n",
        "            print(f\"   🚗 Flujo promedio: {stats['flujo_promedio']:.2f} ± {stats['flujo_std']:.2f}\")\n",
        "            print(f\"   ⏱️  Ocupación promedio: {stats['ocupacion_promedio']:.2f} ± {stats['ocupacion_std']:.2f} ms\")\n",
        "            print(f\"   📋 Registros: {stats['registros']}\")\n",
        "            print(f\"   🛣️  Pistas únicas: {stats['pistas_unicas']}\")\n",
        "\n",
        "        # Análisis de cambios\n",
        "        if 2024 in results and 2025 in results:\n",
        "            print(f\"\\n🔄 CAMBIOS 2024 → 2025:\")\n",
        "\n",
        "            flujo_change = ((results[2025]['flujo_promedio'] - results[2024]['flujo_promedio']) /\n",
        "                           results[2024]['flujo_promedio'] * 100)\n",
        "            ocupacion_change = ((results[2025]['ocupacion_promedio'] - results[2024]['ocupacion_promedio']) /\n",
        "                               results[2024]['ocupacion_promedio'] * 100)\n",
        "\n",
        "            print(f\"   🚗 Cambio en flujo: {flujo_change:+.2f}%\")\n",
        "            print(f\"   ⏱️  Cambio en ocupación: {ocupacion_change:+.2f}%\")\n",
        "\n",
        "            # Interpretación\n",
        "            if flujo_change > 5:\n",
        "                print(\"   📈 AUMENTO SIGNIFICATIVO en el flujo vehicular\")\n",
        "            elif flujo_change < -5:\n",
        "                print(\"   📉 DISMINUCIÓN SIGNIFICATIVA en el flujo vehicular\")\n",
        "            else:\n",
        "                print(\"   ➡️  Flujo vehicular ESTABLE\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def ml_traffic_prediction(self):\n",
        "        \"\"\"Modelo de ML para predicción de tráfico\"\"\"\n",
        "        print(\"\\n🤖 MODELO DE MACHINE LEARNING PARA PREDICCIÓN\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Preparar datos para ML\n",
        "        ml_data = self.data[self.data['es_outlier'] == False].copy()\n",
        "\n",
        "        # Features engineering\n",
        "        features = [\n",
        "            'hora_num', 'minuto', 'dia_semana', 'mes', 'dia',\n",
        "            'Pista', 'es_hora_punta', 'es_fin_semana'\n",
        "        ]\n",
        "\n",
        "        # Ensure 'Pista' is numeric before one-hot encoding\n",
        "        ml_data['Pista'] = pd.to_numeric(ml_data['Pista'], errors='coerce')\n",
        "        ml_data.dropna(subset=['Pista'], inplace=True)\n",
        "\n",
        "\n",
        "        # Crear variables dummy para categóricas\n",
        "        ml_data_encoded = pd.get_dummies(ml_data, columns=['Pista'], prefix='pista')\n",
        "\n",
        "        # Actualizar features con las nuevas columnas\n",
        "        pista_cols = [col for col in ml_data_encoded.columns if col.startswith('pista_')]\n",
        "        features_final = ['hora_num', 'minuto', 'dia_semana', 'mes', 'dia',\n",
        "                         'es_hora_punta', 'es_fin_semana'] + pista_cols\n",
        "\n",
        "        # Ensure all features exist and are numeric\n",
        "        for col in features_final:\n",
        "            if col not in ml_data_encoded.columns:\n",
        "                ml_data_encoded[col] = 0\n",
        "            else:\n",
        "                ml_data_encoded[col] = pd.to_numeric(ml_data_encoded[col], errors='coerce')\n",
        "\n",
        "        ml_data_encoded.dropna(subset=features_final, inplace=True)\n",
        "\n",
        "\n",
        "        X = ml_data_encoded[features_final]\n",
        "        y_flujo = ml_data_encoded['Flujo']\n",
        "        y_ocupacion = ml_data_encoded['Ocupación']\n",
        "\n",
        "        # Check if there's enough data after cleaning\n",
        "        if len(X) == 0:\n",
        "            print(\"❌ No hay suficientes datos válidos para entrenar el modelo de ML.\")\n",
        "            return None, None, None\n",
        "\n",
        "\n",
        "        # Dividir datos\n",
        "        X_train, X_test, y_flujo_train, y_flujo_test, y_ocupacion_train, y_ocupacion_test = train_test_split(\n",
        "            X, y_flujo, y_ocupacion, test_size=0.2, random_state=42\n",
        "        )\n",
        "\n",
        "        # Entrenar modelos\n",
        "        print(\"🎯 Entrenando modelos Random Forest...\")\n",
        "\n",
        "        # Modelo para flujo\n",
        "        rf_flujo = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "        rf_flujo.fit(X_train, y_flujo_train)\n",
        "\n",
        "        # Modelo para ocupación\n",
        "        rf_ocupacion = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "        rf_ocupacion.fit(X_train, y_ocupacion_train)\n",
        "\n",
        "        # Predicciones\n",
        "        y_flujo_pred = rf_flujo.predict(X_test)\n",
        "        y_ocupacion_pred = rf_ocupacion.predict(X_test)\n",
        "\n",
        "        # Métricas\n",
        "        print(f\"\\n📊 MÉTRICAS DEL MODELO:\")\n",
        "        print(f\"   🚗 FLUJO:\")\n",
        "        print(f\"      R²: {r2_score(y_flujo_test, y_flujo_pred):.4f}\")\n",
        "        print(f\"      MAE: {mean_absolute_error(y_flujo_test, y_flujo_pred):.4f}\")\n",
        "        print(f\"      RMSE: {np.sqrt(mean_squared_error(y_flujo_test, y_flujo_pred)):.4f}\")\n",
        "\n",
        "        print(f\"   ⏱️  OCUPACIÓN:\")\n",
        "        print(f\"      R²: {r2_score(y_ocupacion_test, y_ocupacion_pred):.4f}\")\n",
        "        print(f\"      MAE: {mean_absolute_error(y_ocupacion_test, y_ocupacion_pred):.4f}\")\n",
        "        print(f\"      RMSE: {np.sqrt(mean_squared_error(y_ocupacion_test, y_ocupacion_pred)):.4f}\")\n",
        "\n",
        "        # Importancia de features\n",
        "        self._plot_feature_importance(rf_flujo, features_final, \"Flujo\")\n",
        "        self._plot_feature_importance(rf_ocupacion, features_final, \"Ocupación\")\n",
        "\n",
        "        return rf_flujo, rf_ocupacion, features_final\n",
        "\n",
        "    def _plot_feature_importance(self, model, features, target_name):\n",
        "        \"\"\"Visualiza la importancia de las características\"\"\"\n",
        "        importances = model.feature_importances_\n",
        "        indices = np.argsort(importances)[::-1]\n",
        "\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.title(f'Importancia de Características - {target_name}')\n",
        "        plt.bar(range(len(importances)), importances[indices])\n",
        "        plt.xticks(range(len(importances)), [features[i] for i in indices], rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def clustering_analysis(self):\n",
        "        \"\"\"Análisis de clustering para identificar patrones\"\"\"\n",
        "        print(\"\\n🔍 ANÁLISIS DE CLUSTERING - PATRONES DE TRÁFICO\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Preparar datos para clustering\n",
        "        cluster_data = self.data[self.data['es_outlier'] == False].copy()\n",
        "\n",
        "        # Ensure 'Pista' is numeric before grouping\n",
        "        cluster_data['Pista'] = pd.to_numeric(cluster_data['Pista'], errors='coerce')\n",
        "        cluster_data.dropna(subset=['Pista'], inplace=True)\n",
        "\n",
        "\n",
        "        # Agrupar por hora y calcular promedios\n",
        "        hourly_patterns = cluster_data.groupby(['hora_num', 'Pista']).agg({\n",
        "            'Flujo': 'mean',\n",
        "            'Ocupación': 'mean'\n",
        "        }).reset_index()\n",
        "\n",
        "        if hourly_patterns.empty:\n",
        "             print(\"❌ No hay suficientes datos válidos para el análisis de clustering.\")\n",
        "             return None, None\n",
        "\n",
        "\n",
        "        # Aplicar K-means\n",
        "        features_cluster = ['hora_num', 'Flujo', 'Ocupación']\n",
        "        X_cluster = hourly_patterns[features_cluster]\n",
        "\n",
        "        # Normalizar datos\n",
        "        X_cluster_scaled = self.scaler.fit_transform(X_cluster)\n",
        "\n",
        "        # Encontrar número óptimo de clusters (método del codo)\n",
        "        inertias = []\n",
        "        K_range = range(2, min(10, len(X_cluster_scaled) + 1)) # Adjust K_range based on data size\n",
        "\n",
        "\n",
        "        for k in K_range:\n",
        "            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10) # Add n_init\n",
        "            kmeans.fit(X_cluster_scaled)\n",
        "            inertias.append(kmeans.inertia_)\n",
        "\n",
        "        # Aplicar clustering con k óptimo (asumimos k=4 para este ejemplo)\n",
        "        optimal_k = 4\n",
        "        if len(X_cluster_scaled) < optimal_k:\n",
        "             print(f\"⚠️ No hay suficientes puntos de datos para {optimal_k} clusters. Usando {len(X_cluster_scaled)} clusters.\")\n",
        "             optimal_k = len(X_cluster_scaled)\n",
        "             if optimal_k < 2:\n",
        "                 print(\"❌ No hay suficientes datos para realizar clustering.\")\n",
        "                 return None, None\n",
        "\n",
        "        kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10) # Add n_init\n",
        "        hourly_patterns['cluster'] = kmeans.fit_predict(X_cluster_scaled)\n",
        "\n",
        "        # Visualizar clusters\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "        fig.suptitle('Análisis de Clustering - Patrones de Tráfico', fontsize=16)\n",
        "\n",
        "        # Método del codo\n",
        "        axes[0, 0].plot(K_range, inertias, 'bo-')\n",
        "        axes[0, 0].set_xlabel('Número de Clusters')\n",
        "        axes[0, 0].set_ylabel('Inercia')\n",
        "        axes[0, 0].set_title('Método del Codo')\n",
        "        axes[0, 0].grid(True)\n",
        "\n",
        "        # Scatter plot de clusters\n",
        "        scatter = axes[0, 1].scatter(hourly_patterns['Flujo'], hourly_patterns['Ocupación'],\n",
        "                                    c=hourly_patterns['cluster'], cmap='viridis')\n",
        "        axes[0, 1].set_xlabel('Flujo Promedio')\n",
        "        axes[0, 1].set_ylabel('Ocupación Promedio')\n",
        "        axes[0, 1].set_title('Clusters de Tráfico')\n",
        "        plt.colorbar(scatter, ax=axes[0, 1])\n",
        "\n",
        "        # Patrones por hora\n",
        "        for cluster in range(optimal_k):\n",
        "            cluster_data_plot = hourly_patterns[hourly_patterns['cluster'] == cluster]\n",
        "            axes[1, 0].plot(cluster_data_plot['hora_num'], cluster_data_plot['Flujo'],\n",
        "                           label=f'Cluster {cluster}', marker='o')\n",
        "\n",
        "        axes[1, 0].set_xlabel('Hora del día')\n",
        "        axes[1, 0].set_ylabel('Flujo Promedio')\n",
        "        axes[1, 0].set_title('Patrones de Flujo por Cluster')\n",
        "        axes[1, 0].legend()\n",
        "        axes[1, 0].grid(True)\n",
        "\n",
        "        # Distribución de clusters\n",
        "        cluster_counts = hourly_patterns['cluster'].value_counts().sort_index()\n",
        "        axes[1, 1].bar(cluster_counts.index, cluster_counts.values)\n",
        "        axes[1, 1].set_xlabel('Cluster')\n",
        "        axes[1, 1].set_ylabel('Cantidad de Puntos')\n",
        "        axes[1, 1].set_title('Distribución de Clusters')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Análisis de características de cada cluster\n",
        "        print(\"\\n📋 CARACTERÍSTICAS DE CADA CLUSTER:\")\n",
        "        for cluster in range(optimal_k):\n",
        "            cluster_data = hourly_patterns[hourly_patterns['cluster'] == cluster]\n",
        "            print(f\"\\n🏷️  CLUSTER {cluster}:\")\n",
        "            print(f\"   📊 Puntos: {len(cluster_data)}\")\n",
        "            print(f\"   🚗 Flujo promedio: {cluster_data['Flujo'].mean():.2f}\")\n",
        "            print(f\"   ⏱️  Ocupación promedio: {cluster_data['Ocupación'].mean():.2f}\")\n",
        "            print(f\"   🕐 Horas típicas: {cluster_data['hora_num'].min()}-{cluster_data['hora_num'].max()}\")\n",
        "\n",
        "        return hourly_patterns, kmeans\n",
        "\n",
        "    def generate_comprehensive_report(self):\n",
        "        \"\"\"Genera un reporte completo con visualizaciones\"\"\"\n",
        "        print(\"\\n📊 GENERANDO REPORTE COMPLETO\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Resumen general\n",
        "        print(f\"📋 RESUMEN GENERAL:\")\n",
        "        print(f\"   📅 Período: {self.data['Fecha'].min()} - {self.data['Fecha'].max()}\")\n",
        "        print(f\"   📊 Total registros: {len(self.data)}\")\n",
        "        print(f\"   🛣️  Pistas analizadas: {self.data['Pista'].nunique()}\")\n",
        "        print(f\"   🚨 Outliers detectados: {self.data['es_outlier'].sum()}\")\n",
        "\n",
        "        # Estadísticas por año\n",
        "        yearly_stats = self.data.groupby('año').agg({\n",
        "            'Flujo': ['mean', 'std', 'min', 'max'],\n",
        "            'Ocupación': ['mean', 'std', 'min', 'max']\n",
        "        }).round(2)\n",
        "\n",
        "        print(f\"\\n📈 ESTADÍSTICAS POR AÑO:\")\n",
        "        print(yearly_stats)\n",
        "\n",
        "        # Crear visualizaciones\n",
        "        self._create_visualizations()\n",
        "\n",
        "    def _create_visualizations(self):\n",
        "        \"\"\"Crea visualizaciones comprehensivas\"\"\"\n",
        "\n",
        "        # Configurar el estilo\n",
        "        plt.style.use('seaborn-v0_8')\n",
        "\n",
        "        # 1. Distribución temporal del tráfico\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(20, 15))\n",
        "        fig.suptitle('Análisis Temporal del Tráfico Vehicular', fontsize=16)\n",
        "\n",
        "        # Flujo por hora del día\n",
        "        hourly_flow = self.data.groupby('hora_num')['Flujo'].mean()\n",
        "        axes[0, 0].plot(hourly_flow.index, hourly_flow.values, marker='o', linewidth=2)\n",
        "        axes[0, 0].axvspan(7, 9, alpha=0.3, color='red', label='Hora punta mañana')\n",
        "        axes[0, 0].axvspan(17, 20, alpha=0.3, color='red', label='Hora punta tarde')\n",
        "        axes[0, 0].set_xlabel('Hora del día')\n",
        "        axes[0, 0].set_ylabel('Flujo promedio')\n",
        "        axes[0, 0].set_title('Flujo vehicular por hora del día')\n",
        "        axes[0, 0].legend()\n",
        "        axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Ocupación por hora del día\n",
        "        hourly_occ = self.data.groupby('hora_num')['Ocupación'].mean()\n",
        "        axes[0, 1].plot(hourly_occ.index, hourly_occ.values, marker='s', linewidth=2, color='orange')\n",
        "        axes[0, 1].axvspan(7, 9, alpha=0.3, color='red')\n",
        "        axes[0, 1].axvspan(17, 20, alpha=0.3, color='red')\n",
        "        axes[0, 1].set_xlabel('Hora del día')\n",
        "        axes[0, 1].set_ylabel('Ocupación promedio (ms)')\n",
        "        axes[0, 1].set_title('Ocupación por hora del día')\n",
        "        axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "        # Flujo por día de la semana\n",
        "        daily_flow = self.data.groupby('dia_semana')['Flujo'].mean()\n",
        "        days = ['Lun', 'Mar', 'Mié', 'Jue', 'Vie', 'Sáb', 'Dom']\n",
        "        axes[1, 0].bar(daily_flow.index, daily_flow.values, color='skyblue') # Use index for positions\n",
        "        axes[1, 0].set_xticks(daily_flow.index) # Set tick positions\n",
        "        axes[1, 0].set_xticklabels(days) # Set tick labels\n",
        "        axes[1, 0].set_xlabel('Día de la semana')\n",
        "        axes[1, 0].set_ylabel('Flujo promedio')\n",
        "        axes[1, 0].set_title('Flujo vehicular por día de la semana')\n",
        "        axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Comparación por pista\n",
        "        pista_stats = self.data.groupby('Pista')['Flujo'].mean().sort_values(ascending=False)\n",
        "        axes[1, 1].bar(pista_stats.index.astype(str), pista_stats.values, color='lightgreen')\n",
        "        axes[1, 1].set_xlabel('Pista')\n",
        "        axes[1, 1].set_ylabel('Flujo promedio')\n",
        "        axes[1, 1].set_title('Flujo promedio por pista')\n",
        "        axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # 2. Heatmap de tráfico por hora y día\n",
        "        pivot_data = self.data.pivot_table(\n",
        "            values='Flujo',\n",
        "            index='hora_num',\n",
        "            columns='dia_semana',\n",
        "            aggfunc='mean'\n",
        "        )\n",
        "\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        sns.heatmap(pivot_data, annot=True, fmt='.1f', cmap='YlOrRd',\n",
        "                   xticklabels=days, yticklabels=range(24))\n",
        "        plt.title('Heatmap: Flujo vehicular por hora y día de la semana')\n",
        "        plt.xlabel('Día de la semana')\n",
        "        plt.ylabel('Hora del día')\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNFxXPORp5b2",
        "outputId": "bde0a65d-e2f7-41a7-b49f-427771163381"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. FUNCIÓN PRINCIPAL DE EJECUCIÓN\n",
        "def main():\n",
        "    \"\"\"Función principal que ejecuta todo el análisis\"\"\"\n",
        "    print(\"🚀 INICIANDO ANÁLISIS DE TRÁFICO VEHICULAR CON ML\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Inicializar analizador\n",
        "    analyzer = TrafficAnalyzer()\n",
        "\n",
        "    # Cargar datos\n",
        "    analyzer.load_all_excel_files()\n",
        "\n",
        "    if analyzer.data is None:\n",
        "        print(\"❌ No se pudieron cargar los datos. Verifica la ruta.\")\n",
        "        return\n",
        "\n",
        "    # Ejecutar análisis\n",
        "    print(\"\\n🔍 EJECUTANDO ANÁLISIS...\")\n",
        "\n",
        "    # 1. Análisis comparativo básico\n",
        "    comparison_results = analyzer.analyze_peak_hours_comparison()\n",
        "\n",
        "    # 2. Modelo de ML\n",
        "    rf_flujo, rf_ocupacion, features = analyzer.ml_traffic_prediction()\n",
        "\n",
        "    # 3. Análisis de clustering\n",
        "    patterns, kmeans_model = analyzer.clustering_analysis()\n",
        "\n",
        "    # 4. Reporte completo\n",
        "    analyzer.generate_comprehensive_report()\n",
        "\n",
        "    print(\"\\n✅ ANÁLISIS COMPLETADO\")\n",
        "    print(\"📊 Todos los resultados han sido generados exitosamente\")\n",
        "\n",
        "    return analyzer, rf_flujo, rf_ocupacion, patterns, kmeans_model"
      ],
      "metadata": {
        "id": "VQ9uzhUjpzYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. FUNCIÓN DE PREDICCIÓN PARA NUEVOS DATOS\n",
        "def predict_traffic(analyzer, rf_flujo, rf_ocupacion, features, hora, pista, dia_semana, mes=1):\n",
        "    \"\"\"Función para predecir tráfico en condiciones específicas\"\"\"\n",
        "    # Crear DataFrame con las características\n",
        "    new_data = pd.DataFrame({\n",
        "        'hora_num': [hora],\n",
        "        'minuto': [0],\n",
        "        'dia_semana': [dia_semana],\n",
        "        'mes': [mes],\n",
        "        'dia': [1],\n",
        "        'es_hora_punta': [hora in range(7, 9) or hora in range(17, 20)],\n",
        "        'es_fin_semana': [dia_semana in [5, 6]]\n",
        "    })\n",
        "\n",
        "    # Agregar columnas de pista (one-hot encoding)\n",
        "    for feature in features:\n",
        "        if feature.startswith('pista_'):\n",
        "            pista_num = feature.split('_')[1]\n",
        "            new_data[feature] = [1 if pista_num == str(pista) else 0]\n",
        "\n",
        "    # Asegurar que tenemos todas las columnas necesarias\n",
        "    for feature in features:\n",
        "        if feature not in new_data.columns:\n",
        "            new_data[feature] = [0]\n",
        "\n",
        "    # Ordenar columnas según el orden de entrenamiento\n",
        "    new_data = new_data[features]\n",
        "\n",
        "    # Realizar predicción\n",
        "    flujo_pred = rf_flujo.predict(new_data)[0]\n",
        "    ocupacion_pred = rf_ocupacion.predict(new_data)[0]\n",
        "\n",
        "    return flujo_pred, ocupacion_pred"
      ],
      "metadata": {
        "id": "Bh8OOK7MpwI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. EJECUTAR ANÁLISIS\n",
        "if __name__ == \"__main__\":\n",
        "    # Ejecutar análisis principal\n",
        "    analyzer, rf_flujo, rf_ocupacion, patterns, kmeans_model = main()\n",
        "\n",
        "    # Ejemplo de predicción\n",
        "    print(\"\\n🔮 EJEMPLO DE PREDICCIÓN:\")\n",
        "    flujo_pred, ocupacion_pred = predict_traffic(\n",
        "        analyzer, rf_flujo, rf_ocupacion,\n",
        "        analyzer.ml_traffic_prediction()[2],  # features\n",
        "        hora=8,  # 8 AM\n",
        "        pista=1,\n",
        "        dia_semana=1,  # Martes\n",
        "        mes=7  # Julio\n",
        "    )\n",
        "\n",
        "    print(f\"📊 Predicción para Pista 1, Martes 8:00 AM:\")\n",
        "    print(f\"   🚗 Flujo estimado: {flujo_pred:.2f} vehículos\")\n",
        "    print(f\"   ⏱️  Ocupación estimada: {ocupacion_pred:.2f} ms\")"
      ],
      "metadata": {
        "id": "g-0sCDFcptOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5de1e5cc"
      },
      "source": [
        "# 5. EJECUTAR ANÁLISIS\n",
        "if __name__ == \"__main__\":\n",
        "    # Ejecutar análisis principal\n",
        "    analyzer, rf_flujo, rf_ocupacion, patterns, kmeans_model = main()\n",
        "\n",
        "    # Ejemplo de predicción\n",
        "    print(\"\\n🔮 EJEMPLO DE PREDICCIÓN:\")\n",
        "    # Ensure features are passed correctly\n",
        "    flujo_pred, ocupacion_pred = predict_traffic(\n",
        "        analyzer, rf_flujo, rf_ocupacion,\n",
        "        analyzer.ml_traffic_prediction()[2],  # features\n",
        "        hora=8,  # 8 AM\n",
        "        pista=1,\n",
        "        dia_semana=1,  # Martes\n",
        "        mes=7  # Julio\n",
        "    )\n",
        "\n",
        "    print(f\"📊 Predicción para Pista 1, Martes 8:00 AM:\")\n",
        "    print(f\"   🚗 Flujo estimado: {flujo_pred:.2f} vehículos\")\n",
        "    print(f\"   ⏱️  Ocupación estimada: {ocupacion_pred:.2f} ms\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
